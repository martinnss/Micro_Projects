{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos de un proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* importar paquetes y datos\n",
    "* data.head data.columns, shape, dtypes, data.describe()\n",
    "* verificar valores NaN  o missing \n",
    "    * dropna(axis=1,thresh=3)      #borra todas las columnas que tengan desde 3 valores na \n",
    "    * isnull() data.drop()\n",
    "    * data[data['col1'].notnull()] \n",
    "    * data[data['xd']==78]]\n",
    "* Convert dtypes   #entender bien el dataset para saber que tipos de datos debemos usar\n",
    "    * df['col1']=df['col1'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* eliminar columnas irrelevamtes \n",
    "    * Usar t-SNE para visualizar high dimensional data\n",
    "        * Antes se deben selecionar solo las columnas numericas y dejar un dataset df_numeric\n",
    "        * learning_rate=np.arange(10,10000,10) * ejemplo\n",
    "        * agregar al dataset original las dos columnas resultantes df[x], df[y]\n",
    "        * visualizar las columnas resulantes en un scatterplot con hue=categorical_columns\n",
    "    * buscar las columnas con menor varianza y eliminarlas usando VarianceThreshold (apunte dimensionalyty reduction datacmp)\n",
    "* EDA\n",
    "    * \n",
    "* Feature extraction\n",
    "    * Scale data(fit transform, standard sclaer, log normalziation, minmax scaler) detallado en FE apunte\n",
    "\n",
    "\n",
    "\n",
    "* Feature Engineering (preprocessing apunte)\n",
    "    * Binary enconding\n",
    "    * one-hot encoder (pd.get_dummies())\n",
    "* Feature engineering (F.E apunte)\n",
    "    * categorical: cuando hay muchos resultados de variables que no se repiten, pomner 'others'\n",
    "        * Fill categorical values with the mode\n",
    "    * numerical\n",
    "        * Si buscamos saber la std, variance, covariance, test statiscs NO debemos llenar valores aun}\n",
    "        * fill values with mean or median (df['col'].fillna(df['col'].mean().astype('int64'))\n",
    "    * reemplazar valores con str.replace('$','').astype('')    o     pd.to_numeric(df['col'], errors='coerce')\n",
    "    * remove outliers\n",
    "        * ejemplo top 95%\n",
    "        * eliminar todos lo valores que se alejen 3 stds from the mean\n",
    "\n",
    "\n",
    "* feature selection:\n",
    "    * pairplot\n",
    "    * buscar las columnas con menor varianza y eliminarlas usando VarianceThreshold (apunte dimensionalyty reduction datacmp)\n",
    "    * Redundant features (preprocessing apunte)\n",
    "        * noisy, correlated and duplicated features. Usando una mask y corr()\n",
    "        * Identify a column where the correlation value is higher between several features and store it in the to_drop variable.\n",
    "* PCA (last step) (dimensionality reduction apunte)\n",
    "    * Fit PCA and study their components (en casod e tener muchas features)\n",
    "    * Tansfromed_X to train_test_split(X_transformed,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Si se va a reutilizar un algoritmo ya entrenado con nuevos datos, revisar el apunte de Feature Egiuneering\n",
    "\n",
    "* X_train, X_test,y_train, y_test = train_test_split(X,y, stratify=y)\n",
    "    * PCA\n",
    "\n",
    "* hyperparameter tunning\n",
    "    * RandomizedSearchCV \n",
    "        * estimator (algorithm), params={'param1':[],..}, n_iter,cv\n",
    "        * Fit the estimator\n",
    "        * rs.best_score_, rs.best_params_, rs.best_estimator_, rs.cv_results_['mean_test_score']  hay mas, rs.cv_results['params']\n",
    "        * agrupar los resultaods\n",
    "    * Bayesian Optimization (aprender)\n",
    "\n",
    "* Fit the model con best_params\n",
    "    * for linear ver feature importances    \n",
    "    * use cross validation\n",
    "        * for linear use make_scorer o para cualquier ptra metrica \n",
    "    * use Leave one Out cross validation para:\n",
    "        * limited training data\n",
    "        * ciudado cuando: computational resouerces are limited, lot of data, a lot of parameters\n",
    "      \n",
    "    \n",
    "* analyze the metrics\n",
    "    * If accuracy_score(ytest,model.predict(Xtest)) << accuracy_score(ytrain,model.predict(Xtrain)) EXISTE OVERFITTING\n",
    "    para disminuir el efecto del overfitting debemos buscar solamente las columnas mas representativas del dataset\n",
    "        * cambiar hyperparameters \n",
    "    * predict proba\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model validation\n",
    "* siempre ocupar predict con el test set\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
